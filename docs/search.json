[{"path":"/articles/dml-401k.html","id":"load-data","dir":"Articles","previous_headings":"","what":"Load Data","title":"Sensitivity 401k","text":"","code":"# loads package library(dml.sensemakr)  ## loads data data(\"pension\") y <- pension$net_tfa # net total financial assets d <- pension$e401 # 401K eligibility x <- model.matrix(~ -1 + age + inc + educ+ fsize + marr + twoearn + pira + hown, data = pension)"},{"path":"/articles/dml-401k.html","id":"naive-estimate-no-adjustment","dir":"Articles","previous_headings":"","what":"Naive Estimate (no adjustment)","title":"Sensitivity 401k","text":"","code":"mean(y[d==1]) - mean(y[d==0]) #> [1] 19559.34"},{"path":"/articles/dml-401k.html","id":"partially-linear-model","dir":"Articles","previous_headings":"","what":"Partially Linear Model","title":"Sensitivity 401k","text":"","code":"# run DML dml.401k.plm <- dml(y, d, x, model = \"plm\", cf.folds = 5, cf.reps = 5) #> Debiased Machine Learning #>  #>  Model: Partially Linear  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (partially linear). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5 # results under conditional ignorability summary(dml.401k.plm) #>  #> Debiased Machine Learning #>  #>  Model: Partially Linear  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger, R2 = 26.701%), treatment (ranger, R2 = 11.415%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   8974.3     1342.8  6.6835 2.334e-11 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method. # robustness values robustness_value(dml.401k.plm, alpha = 1) # RV point estimate #>        ate  #> 0.07247987 robustness_value(dml.401k.plm, alpha = 0.05) # RV confidence bound #>        ate  #> 0.05435687 # confidence bounds under posited scenario confidence_bounds(dml.401k.plm, r2ya.dx = 0.04, r2.rr = 0.03, level = 0.95) #>           lwr       upr #> ate  2543.509 15470.327 #>  #> Confidence level: point = 95%; region = 90%. #> Sensitivity parameters: r2ya.dx = 0.04; r2.rr = 0.03; rho2 = 1. # sensitivity contour plots par(mfrow = c(1,2)) ovb_contour_plot(dml.401k.plm, which.bound = \"lwr\", r2ya.dx = 0.04, r2.rr = 0.03,                   bound.label = \"Max match (3 years)\", col.contour = \"blue\") ovb_contour_plot(dml.401k.plm, which.bound = \"upr\", r2ya.dx = 0.04, r2.rr = 0.03,                   bound.label = \"Max match (3 years)\", col.contour = \"blue\") # benchmarks bench.plm <- dml_benchmark(dml.401k.plm, benchmark_covariates = c(\"inc\", \"pira\", \"twoearn\")) #>  #> === Computing benchmarks using covariate: inc  === #>  #> Debiased Machine Learning #>  #>  Model: Partially Linear  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (partially linear). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5   #>  #>  #> === Computing benchmarks using covariate: pira  === #>  #> Debiased Machine Learning #>  #>  Model: Partially Linear  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (partially linear). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5   #>  #>  #> === Computing benchmarks using covariate: twoearn  === #>  #> Debiased Machine Learning #>  #>  Model: Partially Linear  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (partially linear). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5 bench.plm #>          gain.Y   gain.D     rho theta.s theta.sj  delta #> inc     0.14493 0.047465  0.3478    8988    12342 3353.7 #> pira    0.04568 0.006203  0.3042    8988     9498  510.2 #> twoearn 0.01381 0.006955 -0.4858    8988     8474 -514.4"},{"path":"/articles/dml-401k.html","id":"nonparametric-model","dir":"Articles","previous_headings":"","what":"Nonparametric Model","title":"Sensitivity 401k","text":"","code":"## compute income quartiles g1 <- cut(x[,\"inc\"], quantile(x[,\"inc\"], c(0, 0.25,.5,.75,1), na.rm = TRUE),            labels = c(\"q1\", \"q2\", \"q3\", \"q4\"), include.lowest = T)  ## Nonparametric model dml.401k.npm <- dml(y, d, x, groups = g1, model = \"npm\", cf.folds = 5, cf.reps = 5) #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    3             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5 # results under conditional ignorability summary(dml.401k.npm) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger, R2 = 26.693%), treatment (ranger, R2 = 11.555%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   7824.9     1236.3  6.3293 2.462e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Group Average Treatment Effect:  #>  #>         Estimate Std. Error t value   P(>|t|)     #> gate.q1  4269.23     888.46  4.8052 1.546e-06 *** #> gate.q2  2759.35    1392.00  1.9823 0.0474478 *   #> gate.q3  6677.69    1900.88  3.5129 0.0004432 *** #> gate.q4 17733.98    4264.69  4.1583 3.206e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Note: DML estimates combined using the median method. # load ggplot2 library(ggplot2)  # coefficient plot under conditional ignorability df   <- data.frame(groups = 1:4, estimate = coef(dml.401k.npm)[-1]) cis  <- confint(dml.401k.npm)[-1, ] cis  <- setNames(as.data.frame(cis), c(\"lwr.ci\", \"upr.ci\")) df   <- cbind(df, cis) ggplot(df, aes(x = groups, y = estimate)) + geom_line() +   geom_ribbon(aes(ymin = lwr.ci, ymax = upr.ci), alpha = 0.1, col = \"blue\", fill = \"blue\") +   theme_bw() +    xlab(\"Income Groups by Quartiles\") +    ylab(\"ATE\") # robustness values robustness_value(dml.401k.npm, alpha = 1) # RV point estimate #>        ate    gate.q1    gate.q2    gate.q3    gate.q4  #> 0.05987821 0.09966737 0.04510139 0.06662249 0.08967410 robustness_value(dml.401k.npm, alpha = 0.05) # RV confidence bound #>         ate     gate.q1     gate.q2     gate.q3     gate.q4  #> 0.043812408 0.066854463 0.006776972 0.033765900 0.054790424 # bounds and confidence bounds  bds   <- confidence_bounds(dml.401k.npm, r2ya.dx = 0.04, r2.rr = 0.03, level = 0) bds   <- setNames(as.data.frame(bds), c(\"lwr.bound\", \"upr.bound\")) cbds  <- confidence_bounds(dml.401k.npm, r2ya.dx = 0.04, r2.rr = 0.03, level = .95) cbds  <- setNames(as.data.frame(cbds), c(\"lwr.cbound\", \"upr.cbound\")) cbind(bds, cbds) #>          lwr.bound upr.bound  lwr.cbound upr.cbound #> ate      3368.2883 12313.881  1264.11968  14422.809 #> gate.q1  2836.8292  5701.629  1388.09033   7195.554 #> gate.q2   664.2309  4851.132 -2021.47222   6909.304 #> gate.q3  3271.7355 10083.644   -87.71927  13239.922 #> gate.q4 11097.4321 24370.536  4070.36407  31509.697 # confidence bounds plot df2   <- cbind(df, bds[-1,], cbds[-1, ]) ggplot(df2, aes(x = groups, y = estimate)) + geom_line() +   geom_ribbon(aes(ymin = lwr.bound, ymax = upr.bound),   alpha = 0.1, col = \"red\", fill = \"red\") +   geom_ribbon(aes(ymin = lwr.cbound, ymax = upr.cbound), alpha = 0.1, col = \"blue\", fill = \"blue\") +   theme_bw() +    xlab(\"Income Groups by Quartiles\") +    ylab(\"ATE\") # sensitivity contour plots par(mfrow = c(1,2)) ovb_contour_plot(dml.401k.npm, which.bound = \"lwr\", r2ya.dx = 0.04, r2.rr = 0.03,                   bound.label = \"Max match (3 years)\", col.contour = \"blue\") ovb_contour_plot(dml.401k.npm, which.bound = \"upr\", r2ya.dx = 0.04, r2.rr = 0.03,                   bound.label = \"Max match (3 years)\", col.contour = \"blue\") # benchmarks bench.npm <- dml_benchmark(dml.401k.npm, benchmark_covariates = c(\"inc\", \"pira\", \"twoearn\")) #>  #> === Computing benchmarks using covariate: inc  === #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5   #>  #>  #> === Computing benchmarks using covariate: pira  === #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5   #>  #>  #> === Computing benchmarks using covariate: twoearn  === #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 5 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 5 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5   #>  #> -- Rep 2 -- Folds: 1  2  3  4  5   #>  #> -- Rep 3 -- Folds: 1  2  3  4  5   #>  #> -- Rep 4 -- Folds: 1  2  3  4  5   #>  #> -- Rep 5 -- Folds: 1  2  3  4  5 bench.npm #>           gain.Y   gain.D    rho theta.s theta.sj  delta #> inc     0.128267 0.127385 0.2505    7881    11689 3808.0 #> pira    0.034770 0.001702 0.8589    7881     8541  659.7 #> twoearn 0.006894 0.001100 0.0000    7881     7694 -186.3"},{"path":"/articles/other-ml-methods.html","id":"quick-overview","dir":"Articles","previous_headings":"","what":"Quick Overview","title":"Using Different ML Methods","text":"Users can provide different ML methods dml() using reg argument. name method provided, tuning performed, default parameters used. instance, code runs DML using generalized additive models (GAMs). code uses gradient boosting machines (GBMs). used ML method estimating regression treatment outcome. Note, however, can use different methods regression, specifying yreg dreg separately. instance, code uses GAM outcome regression, GBM treatment regression.","code":"# generalized additive model dml.gam <- dml(y, d, x, model = \"npm\", reg = \"gam\") #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gam), treatment (gam) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   select method #> 1   TRUE GCV.Cp #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   select method #> 1   TRUE GCV.Cp #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5 summary(dml.gam) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gam, R2 = 28.292%), treatment (gam, R2 = 12.334%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   7079.6     1138.4  6.2187 5.012e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method. # gradient boosting machine dml.gbm  <- dml(y, d, x,  model = \"npm\", reg = \"gbm\") #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gbm), treatment (gbm) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   interaction.depth n.trees shrinkage n.minobsinnode #> 1                 1      50       0.1             10 #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   interaction.depth n.trees shrinkage n.minobsinnode #> 1                 1      50       0.1             10 #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5 summary(dml.gbm) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gbm, R2 = 23.154%), treatment (gbm, R2 = 11.837%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   7467.8     1166.4  6.4021 1.532e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method. # gradient boosting machine dml.gam.gbm  <- dml(y, d, x,  model = \"npm\", yreg = \"gam\", dreg = \"gbm\") #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gam), treatment (gbm) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   interaction.depth n.trees shrinkage n.minobsinnode #> 1                 1      50       0.1             10 #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   select method #> 1   TRUE GCV.Cp #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5 summary(dml.gam.gbm) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gam, R2 = 27.059%), treatment (gbm, R2 = 11.778%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   7676.7     1114.0  6.8912 5.533e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method."},{"path":"/articles/other-ml-methods.html","id":"tuning-parameters","dir":"Articles","previous_headings":"Quick Overview","what":"Tuning Parameters","title":"Using Different ML Methods","text":"Users can provide details form cross-validation specific tuning grid passing named list arguments via reg. arguments reg include relevant arguments train() function package caret. main arguments : method, trControl tuneGrid tuneLength. See ?caret::train information. instance, code performs 5-fold cross-validation, search parameters grid size 5 GBM (values grid chosen caret).","code":"# gradient boosting machine gbm.args <- list(method = \"gbm\",                   trControl = list(method = \"cv\", number = 5),                  tuneLength  = 5) dml.gbm  <- dml(y, d, x,  model = \"npm\", reg = gbm.args) #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gbm), treatment (gbm) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>    n.trees interaction.depth shrinkage n.minobsinnode #> 16      50                 4       0.1             10 #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>    n.trees interaction.depth shrinkage n.minobsinnode #> 10     250                 2       0.1             10 #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5 summary(dml.gbm) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (gbm, R2 = 26.803%), treatment (gbm, R2 = 12.135%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   8175.7     1150.2  7.1083 1.175e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method."},{"path":"/articles/other-ml-methods.html","id":"other-methods","dir":"Articles","previous_headings":"","what":"Other Methods","title":"Using Different ML Methods","text":"provide templates machine learning methods. examples may change trControl favorite choice cross-validation, instance, trControl = list(method = \"cv\", number = 5) 5-fold cross validation, also expand parameters tuning grid accordingly.","code":""},{"path":"/articles/other-ml-methods.html","id":"neural-networks","dir":"Articles","previous_headings":"Other Methods","what":"Neural Networks","title":"Using Different ML Methods","text":"Template using neural networks.","code":"# Neural Net nnet.args     <- list(method = \"nnet\",                        trControl = list(method  = \"none\"),                        tuneGrid = expand.grid(size = 8, decay = 0.01),                        maxit = 1000, maxNWts = 10000) dml.nnet <- dml(y, d, x, model = \"npm\", reg = nnet.args) #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (nnet), treatment (nnet) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   size decay #> 1    8  0.01 #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   size decay #> 1    8  0.01 #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5 summary(dml.nnet) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (nnet, R2 = 28.732%), treatment (nnet, R2 = 11.995%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   7676.8     1232.0  6.2313 4.627e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method."},{"path":"/articles/other-ml-methods.html","id":"lasso-with-polynomial-expansion","dir":"Articles","previous_headings":"Other Methods","what":"Lasso with Polynomial Expansion","title":"Using Different ML Methods","text":"Template using lasso polynomial expansion covariates x.","code":"# creates polynomial expansion of x xl  <- model.matrix(~ -1 + (poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) +                       poly(educ, 4, raw=TRUE) + poly(fsize, 2, raw=TRUE) +                       marr + twoearn + pira + hown)^2, data = pension) # lasso args lasso.args <- list(method = \"glmnet\",                    trControl = list(method = \"none\"),                    tuneGrid = expand.grid(alpha = 1, lambda = 0.002))  # fit dml dml.glmnet <- dml(y, d, xl, model = \"npm\", reg= lasso.args) #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (glmnet), treatment (glmnet) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   alpha lambda #> 1     1  0.002 #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   alpha lambda #> 1     1  0.002 #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5 summary(dml.glmnet) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (glmnet, R2 = 27.921%), treatment (glmnet, R2 = 11.98%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value P(>|t|) #> ate   5399.6     3570.3  1.5124  0.1304 #> Note: DML estimates combined using the median method."},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Carlos Cinelli. Author, maintainer. Victor Chernozhukov. Author. Vasilis Syrgkanis. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Cinelli C, Chernozhukov V, Syrgkanis V (2023). dml.sensemakr: Sensitivity Analysis Debiased Machine Learning. R package version 0.1.0.","code":"@Manual{,   title = {dml.sensemakr: Sensitivity Analysis for Debiased Machine Learning},   author = {Carlos Cinelli and Victor Chernozhukov and Vasilis Syrgkanis},   year = {2023},   note = {R package version 0.1.0}, }"},{"path":"/index.html","id":"dmlsensemakr","dir":"","previous_headings":"","what":"Sensitivity Analysis for Debiased Machine Learning","title":"Sensitivity Analysis for Debiased Machine Learning","text":"dml.sensemakr implements general suite sensitivity analysis tools Causal Machine Learning discussed Chernozhukov, V., Cinelli, C., Newey, W., Sharma ., Syrgkanis, V. (2021). “Long Story Short: Omitted Variable Bias Causal Machine Learning.”","code":""},{"path":"/index.html","id":"development-version","dir":"","previous_headings":"","what":"Development version","title":"Sensitivity Analysis for Debiased Machine Learning","text":"install development version GitHub make sure package devtools installed.","code":"# install.packages(\"devtools\")  devtools::install_github(\"carloscinelli/dml.sensemakr\")"},{"path":"/index.html","id":"cran","dir":"","previous_headings":"","what":"CRAN","title":"Sensitivity Analysis for Debiased Machine Learning","text":"CRAN version coming soon.","code":""},{"path":"/index.html","id":"details","dir":"","previous_headings":"","what":"Details","title":"Sensitivity Analysis for Debiased Machine Learning","text":"theoretical details please see working paper. primer Debiased Machine Learning, please check Chernozukohv et al (2018). presentations may useful: Carlos’ presentation ICLR. Victor’s tutorial Chamberlain Seminar.","code":""},{"path":"/index.html","id":"basic-usage","dir":"","previous_headings":"","what":"Basic Usage","title":"Sensitivity Analysis for Debiased Machine Learning","text":"","code":"# loads package library(dml.sensemakr) #> See details in: #> - Chernozhukov, V. Cinelli, C. Newey, W. Sharma, A. Syrgkanis, V. (2021). Long Story Short: Omitted Variable Bias in Causal Machine Learning. National Bureau of Economic Research, Working Paper Series, 30302. #> - Available at: http://www.nber.org/papers/w30302  ## loads data data(\"pension\")  # set treatment, outcome and covariates y <- pension$net_tfa  # net total financial assets d <- pension$e401     # 401K eligibility x <- model.matrix(~ -1 + age + inc  + educ+ fsize + marr + twoearn + pira + hown, data = pension)  # run DML (nonparametric model) dml.401k <- dml(y, d, x, model = \"npm\") #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    3             5  variance #>  #>  #> ====================================== #> Repeating 5-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2  3  4  5  # summary of results with median method (default) summary(dml.401k) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 5 folds, 1 reps  #>  ML Method: outcome (ranger, R2 = 26.274%), treatment (ranger, R2 = 11.469%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   8081.6     1171.3  6.8997 5.211e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> Note: DML estimates combined using the median method.  # robustness values robustness_value(dml.401k, alpha = 0.05) #>        ate  #> 0.04628754  # confidence bounds confidence_bounds(dml.401k, r2ya.dx = 0.03, r2.rr = 0.04, level = 0.95) #>           lwr       upr #> ate  1588.546 14628.766 #>  #> Confidence level: point = 95%; region = 90%. #> Sensitivity parameters: r2ya.dx = 0.03; r2.rr = 0.04; rho2 = 1.  # contour plots ovb_contour_plot(dml.401k, r2ya.dx = 0.03, r2.rr = 0.04, bound.label = \"Max Match (3x years)\")"},{"path":"/reference/dml.html","id":null,"dir":"Reference","previous_headings":"","what":"Debiased Machine Learning — dml","title":"Debiased Machine Learning — dml","text":"Estimates target parameter interest, average treatment effect (ATE), using Debiased Machine #Learning (DML). function dml.gate convenience function adds groups dml object model fit.","code":""},{"path":"/reference/dml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Debiased Machine Learning — dml","text":"","code":"dml(   y,   d,   x,   model = c(\"plm\", \"npm\"),   target = \"ate\",   groups = NULL,   cf.folds = 5,   cf.reps = 1,   ps.trim = 0.01,   reg = \"ranger\",   yreg = reg,   dreg = reg,   dirty.tuning = TRUE,   save.models = FALSE,   y.class = FALSE,   d.class = FALSE,   verbose = TRUE,   warnings = FALSE )  dml_gate(dml.fit, groups, ...)"},{"path":"/reference/dml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Debiased Machine Learning — dml","text":"y numeric vector outcome. d numeric vector treatment. treatment binary, needs encoded : zero = absence treatment, one = presence treatment. x numeric vector matrix covariates. suggest constructing x using model.matrix. model specifies model. Current available options plm partially linear model, npm fully non-parametric model. target specifies target causal quantity interest. Current available option ate (ATE - average treatment effect). Note partially linear model continuous treatment ATE also equals average causal derivative (ACD). nonparametric model, ATE available binary treatments. options (eg., ACD nonparametric model, ATT) available soon. groups factor numeric vector indicating group membership. Groups must deterministic function x. cf.folds number cross-fitting folds. Default 2. cf.reps number cross-fitting repetitions. Default 1. ps.trim trims propensity scores lower ps.trim greater 1-ps.trim, order obtain stable estimates. relevant case binary treatment. reg details machine learning method used estimating nuisance parameters (e.g, regression functions treatment outcome). Currently, specified using arguments caret's train function. default random forest using ranger. default method fast usually works well many applications. yreg reg, specifies arguments outcome regression alone. Default value reg. dreg reg, specifies arguments treatment regression alone. Default value reg. dirty.tuning tuning machine learning method happen within cross-fit fold (\"clean\"), using data (\"dirty\")? Default dirty tuning (dirty.tuning = T). long number choices tuning parameters big, dirty tuning faster affect asymptotic guarantees DML. save.models fitted models iterated saved? Default FALSE. Note setting true end using lot memory. y.class y binary, outcome regression treated classification problem? Default FALSE. Note DML need class probabilities, regression gives us . change classification, need make sure method outputs class probabilities. d.class d binary, outcome regression treated classification problem? Default FALSE. Note DML need class probabilities, regression gives us . change classification, need make sure method outputs class probabilities. verbose TRUE (default) prints steps fitting procedure. warning caret's warnings printed? Default FALSE. Note caret many inconsistent unnecessary warnings.","code":""},{"path":"/reference/dml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Debiased Machine Learning — dml","text":"object class dml results DML procedure. object list containing: data list data used. call original call used fit model. info list general information arguments DML fitting procedure. fits list predictions repetition. results list results (influence functions estimates) repetition. coefs list estimates standard errors repetition.","code":""},{"path":"/reference/dml.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Debiased Machine Learning — dml","text":"Chernozhukov, V., Cinelli, C., Newey, W., Sharma ., Syrgkanis, V. (2021). \"Long Story Short: Omitted Variable Bias Causal Machine Learning.\"","code":""},{"path":"/reference/dml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Debiased Machine Learning — dml","text":"","code":"# loads package library(dml.sensemakr)  ## loads data data(\"pension\")  # set the outcome y <- pension$net_tfa  # net total financial assets  # set the treatment d <- pension$e401    # 401K eligibility  # set the covariates (a matrix) x <- model.matrix(~ -1 + age + inc  + educ+ fsize + marr + twoearn + pira + hown, data = pension)  ## compute income quartiles for group ATE. g1 <- cut(x[,\"inc\"], quantile(x[,\"inc\"], c(0, 0.25,.5,.75,1), na.rm = TRUE),          labels = c(\"q1\", \"q2\", \"q3\", \"q4\"), include.lowest = T) # run DML (nonparametric model) ## 2 folds (change as needed) ## 1 repetition (change as needed) dml.401k <- dml(y, d, x, model = \"npm\", groups = g1, cf.folds = 2, cf.reps = 1) #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 2 folds, 1 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    3             5  variance #>  #>  #> ====================================== #> Repeating 2-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2   #>   # summary of results with median method (default) summary(dml.401k, combine.method = \"median\") #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 2 folds, 1 reps  #>  ML Method: outcome (ranger, R2 = 25.519%), treatment (ranger, R2 = 11.154%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value   P(>|t|)     #> ate   7582.0     1130.9  6.7045 2.022e-11 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Group Average Treatment Effect:  #>  #>         Estimate Std. Error t value   P(>|t|)     #> gate.q1  4421.97     788.11  5.6108 2.014e-08 *** #> gate.q2  3316.05    1241.19  2.6717 0.0075472 **  #> gate.q3  7082.60    1869.44  3.7886 0.0001515 *** #> gate.q4 15506.18    3843.09  4.0348 5.464e-05 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Note: DML estimates combined using the median method.  # coef median method (default) coef(dml.401k, combine.method = \"median\") #>       ate   gate.q1   gate.q2   gate.q3   gate.q4  #>  7581.976  4421.968  3316.055  7082.602 15506.184   # se median method (default) se(dml.401k, combine.method = \"median\") #>       ate   gate.q1   gate.q2   gate.q3   gate.q4  #> 1130.8860  788.1139 1241.1861 1869.4439 3843.0900   # confint median method confint(dml.401k, combine.method = \"median\") #>             2.5 %    97.5 % #> ate     5365.4798  9798.472 #> gate.q1 2877.2931  5966.643 #> gate.q2  883.3746  5748.735 #> gate.q3 3418.5595 10746.645 #> gate.q4 7973.8662 23038.502  ## Sensitivity Analysis  ### Robustness Values robustness_value(dml.401k, alpha = 0.05) #>        ate    gate.q1    gate.q2    gate.q3    gate.q4  #> 0.04290545 0.07379381 0.01781195 0.03827998 0.04665653   ### Confidence Bounds confidence_bounds(dml.401k, r2ya.dx = 0.03, r2.rr = 0.04, level = 0.95) #>                lwr        upr #> ate      1126.6569 14037.8911 #> gate.q1  1699.6328  7173.2102 #> gate.q2 -1266.4339  7211.7752 #> gate.q3   390.2901 13640.8427 #> gate.q4  2414.6468 28691.0831 #>  #> Confidence level: point = 95%; region = 90%. #> Sensitivity parameters: r2ya.dx = 0.03; r2.rr = 0.04; rho2 = 1.  ### Contour Plots ovb_contour_plot(dml.401k, r2ya.dx = 0.03, r2.rr = 0.04,                 bound.label = \"Max Match (3x years)\")"},{"path":"/reference/dml_benchmark.html","id":null,"dir":"Reference","previous_headings":"","what":"Benchmarks for the strength of latent variables using observed covariates — dml_benchmark","title":"Benchmarks for the strength of latent variables using observed covariates — dml_benchmark","text":"Compute benchmarks strength latent variables, assumption gains explanatory power due latent variables proportional gains observed covariates.","code":""},{"path":"/reference/dml_benchmark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Benchmarks for the strength of latent variables using observed covariates — dml_benchmark","text":"","code":"dml_benchmark(model, benchmark_covariates)"},{"path":"/reference/dml_benchmark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Benchmarks for the strength of latent variables using observed covariates — dml_benchmark","text":"model object class dml. benchmark_covariates character vector names observed covariates used benchmarking.","code":""},{"path":"/reference/dml_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Bounds on the omitted variable bias for causal machine learning — dml_bounds","title":"Bounds on the omitted variable bias for causal machine learning — dml_bounds","text":"Computes confidence bounds target parameter interest accounting omitted variable biases.","code":""},{"path":"/reference/dml_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bounds on the omitted variable bias for causal machine learning — dml_bounds","text":"","code":"dml_bounds(model, r2ya.dx, r2.rr, rho2 = 1)  confidence_bounds(model, ...)  # S3 method for numeric confidence_bounds(   theta.s,   S2,   se.theta.s,   se.S2,   cov.theta.S2,   r2ya.dx,   r2.rr,   rho2 = 1,   combine.method = \"median\",   level = 0.95 )  # S3 method for dml confidence_bounds(   model,   r2ya.dx,   r2.rr,   rho2 = 1,   level = 0.95,   combine.method = \"median\",   ... )  # S3 method for dml.bounds confidence_bounds(   model,   r2ya.dx = NULL,   r2.rr = NULL,   rho2 = NULL,   level = 0.95,   combine.method = \"median\",   return = c(\"lwr\", \"upr\"),   ... )"},{"path":"/reference/dml_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bounds on the omitted variable bias for causal machine learning — dml_bounds","text":"model object class dml dml.bounds. r2ya.dx (nonparametric) partial R2 omitted variables outcome. Must number (0, 1). r2.rr much variation latent variables create Riesz Representer target parameters. Must number (0, 1). target interest ATE partially linear model, corresponds partial R2 omitted variables treatment. target interest ATE non-parametric model binary treatment, corresponds gains precision (.e, 1/variance) predicting assigned treatment. rho2 degree adversity. Default rho=1, assumes maximum degree adversity confounding.","code":""},{"path":"/reference/ovb_contour_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Contour plots of omitted variable bias for Debiased Machine Learning — ovb_contour_plot","title":"Contour plots of omitted variable bias for Debiased Machine Learning — ovb_contour_plot","text":"Contour plots omitted variable bias sensitivity analysis. main input dml model. vertical axis shows partial R2 omitted variables outcome, .e, maximum proportion residual variation outcome explained latent variables. horizontal axis shows proportion variation long Riesz Representer explained short Riesz Representer (RR). indicates much variation RR created latent variables. partial linear model, quantity paralels sensitivity parameter outcome, simply equals partial R2 latent variables treatment, .e, maximum proportion residual variation treatment explained latent variables. non-parametric model binary treatment, interpretation analagous, instead gains variance, stands gains precision (1/variance). contour levels represent lower (upper) limit confidence bound target interest, considering omitted variables postulated strength. dotted red line shows chosen critical threshold (instance, zero). Almost parameters can customized user.","code":""},{"path":"/reference/ovb_contour_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contour plots of omitted variable bias for Debiased Machine Learning — ovb_contour_plot","text":"","code":"ovb_contour_plot(model, ...)  # S3 method for dml ovb_contour_plot(   model,   which.bound = c(\"lwr\", \"upr\"),   level = 0.95,   combine.method = \"median\",   rho2 = 1,   r2ya.dx = NULL,   r2.rr = r2ya.dx,   bound.label = \"Manual\",   group = FALSE,   group.number = 1,   threshold = 0,   lim.x = 0.15,   lim.y = lim.x,   asp = lim.x/lim.y,   nlevels = 10,   grid.number = 70,   col.contour = \"grey40\",   col.thr.line = \"red\",   xlab = NULL,   ylab = NULL,   cex.lab = 0.8,   cex.axis = 0.8,   cex.main = 1,   show.unadjusted = TRUE,   label.unadjusted = \"Unadjusted\",   label.text = TRUE,   round = 0,   cex.label.text = 0.7,   label.bump.x = NULL,   label.bump.y = NULL,   list.par = NULL )"},{"path":"/reference/ovb_contour_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contour plots of omitted variable bias for Debiased Machine Learning — ovb_contour_plot","text":"model object class dml. ... arguments passed methods. .bound show contours lower limit (lwr) upper limit (upr) confidence bounds? level confidence level. Default 0.95. combine.method method combine results repetition DML fit. Options mean median. Default median. rho2 degree adversity. Default rho=1, assumes maximum degree adversity confounding. r2ya.dx (nonparametric) partial R2 omitted variables outcome. Must number (0, 1). r2.rr much variation latent variables create Riesz Representer target parameters. Must number (0, 1). target interest ATE partially linear model, corresponds partial R2 omitted variables treatment. target interest ATE non-parametric model binary treatment, corresponds gains precision (.e, 1/variance) predicting assigned treatment. bound.label text label manual bound provided via r2ya.dx r2.rr. group contour plots main analysis group analysis? Default FALSE (main analysis). group.number group = TRUE, provide number (level) group. threshold critical threshold interest. Default 0. lim.x sets limit x-axis. `NULL`, limits computed automatically. lim.y sets limit y-axis. `NULL`, limits computed automatically. asp y/x aspect ratio. Default 1. nlevels number levels contour plot. grid.number approximate number grid points axis. col.contour color contour lines. col.thr.line color threshold contour line. xlab label x axis. `NULL`, default label used. ylab label y axis. `NULL`, default label used. cex.lab magnification used x y labels relative current setting cex. cex.axis magnification used axis annotation relative current setting cex. cex.main magnification used main titles relative current setting cex. show.unadjusted unadjusted estimates shown? Default `TRUE`. label.unadjusted label unadjusted estimate. Default \"Unadjusted\". label.text label texts plotted? Default TRUE. round number digits show contours bound values cex.label.text magnification used label text relative current setting cex. label.bump.x bump x coordinate label text. label.bump.y bump y coordinate label text. list.par arguments passed par. needs named list.","code":""},{"path":"/reference/plot.dml.html","id":null,"dir":"Reference","previous_headings":"","what":"Coefficient plots for DML and bounds — plot.dml","title":"Coefficient plots for DML and bounds — plot.dml","text":"Coefficient plots DML bounds","code":""},{"path":"/reference/plot.dml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coefficient plots for DML and bounds — plot.dml","text":"","code":"# S3 method for dml plot(x, combine.method = \"median\", level = 0.95, ...)  # S3 method for dml.bounds plot(   x,   type = c(\"confidence_bounds\", \"all\"),   combine.method = \"median\",   level = 0.95,   ... )"},{"path":"/reference/plot.dml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coefficient plots for DML and bounds — plot.dml","text":"x object class dml dml.bounds. combine.method method combine results repetition DML fit. Options mean median. Default median. level confidence level. Default 0.95. ... arguments passed methods. type type plot confidence bounds. Options confidence_bounds,","code":""},{"path":"/reference/robustness_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes Robustness Values for Debiased Machine Learning — robustness_value","title":"Computes Robustness Values for Debiased Machine Learning — robustness_value","text":"function computes robustness value target parameter estimated via debiased machine learning. robustness value describes minimum strength association (parameterized terms  R2) omitted variables need outcome Riesz Representer confidence bounds target parameter includes zero (another threshold interest).","code":""},{"path":"/reference/robustness_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes Robustness Values for Debiased Machine Learning — robustness_value","text":"","code":"robustness_value(...)  # S3 method for dml robustness_value(model, theta = 0, alpha = 0.05, ...)  # S3 method for dml.bounds robustness_value(model, theta = 0, alpha = 0.05, ...)"},{"path":"/reference/robustness_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes Robustness Values for Debiased Machine Learning — robustness_value","text":"... arguments passed methods. model object class dml dml.bounds. theta null hypothesis interest target parameter theta. Default theta =0 (zero null hypothesis). alpha significance level. Default alpha = 0.05.","code":""},{"path":"/reference/summary.dml.html","id":null,"dir":"Reference","previous_headings":"","what":"coef, se, confint, print and summary methods for DML — summary.dml","title":"coef, se, confint, print and summary methods for DML — summary.dml","text":"print summary methods provide descriptions results obtained function dml. coef function extracts coefficients. se function extracts standard errors. confint function extracts standard errors.","code":""},{"path":"/reference/summary.dml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef, se, confint, print and summary methods for DML — summary.dml","text":"","code":"# S3 method for dml summary(object, combine.method = \"median\", ...)  # S3 method for dml coef(object, combine.method = \"median\", ...)  se(object, ...)  # S3 method for dml se(object, combine.method = \"median\", ...)  # S3 method for dml confint(object, params = NULL, level = 0.95, combine.method = \"median\", ...)  # S3 method for summary_dml print(x, ...)  # S3 method for dml print(   x,   digits = max(3L, getOption(\"digits\") - 3L),   combine.method = \"median\",   ... )"},{"path":"/reference/summary.dml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef, se, confint, print and summary methods for DML — summary.dml","text":"object object class dml. combine.method method combine results repetition DML fit. Options mean median. Default median. ... arguments passed methods. params character vector names parameters. level confidence level. Default 0.95. x object class dml. digits minimal number significant digits.","code":""},{"path":"/reference/summary.dml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"coef, se, confint, print and summary methods for DML — summary.dml","text":"","code":"# loads package library(dml.sensemakr)  ## loads data data(\"pension\")  # set the outcome y <- pension$net_tfa  # net total financial assets  # set the treatment d <- pension$e401    # 401K eligibility  # set the covariates (a matrix) x <- model.matrix(~ -1 + age + inc  + educ+ fsize + marr + twoearn + pira + hown, data = pension)  ## compute income quartiles for group ATE. g1 <- cut(x[,\"inc\"], quantile(x[,\"inc\"], c(0, 0.25,.5,.75,1), na.rm = TRUE),           labels = c(\"q1\", \"q2\", \"q3\", \"q4\"), include.lowest = T)  # run DML (nonparametric model) ## 2 folds (change as needed) ## 1 repetition (change as needed) dml.401k <- dml(y, d, x, model = \"npm\", groups = g1, cf.folds = 2, cf.reps = 1) #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Target: ate  #>  Cross-Fitting: 2 folds, 1 reps  #>  ML Method: outcome (ranger), treatment (ranger) #>  Tuning: dirty  #>  #>  #> ==================================== #> Tuning parameters using all the data #> ==================================== #>  #> - Tuning Model for D. #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    2             5  variance #>  #> - Tuning Model for Y (non-parametric). #> -- Best Tune: #>   mtry min.node.size splitrule #> 1    3             5  variance #>  #>  #> ====================================== #> Repeating 2-fold cross-fitting 1 times #> ====================================== #>  #> -- Rep 1 -- Folds: 1  2   #>    summary(dml.401k) #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 2 folds, 1 reps  #>  ML Method: outcome (ranger, R2 = 27.255%), treatment (ranger, R2 = 11.206%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value  P(>|t|)     #> ate   7847.4     1269.6  6.1812 6.36e-10 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Group Average Treatment Effect:  #>  #>         Estimate Std. Error t value   P(>|t|)     #> gate.q1   4451.9      803.1  5.5434 2.967e-08 *** #> gate.q2   2102.0     1379.6  1.5236 0.1275979     #> gate.q3   7040.3     1844.2  3.8176 0.0001347 *** #> gate.q4  17793.1     4447.5  4.0007 6.316e-05 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Note: DML estimates combined using the median method. summary(dml.401k, combine.method = \"mean\") #>  #> Debiased Machine Learning #>  #>  Model: Nonparametric  #>  Cross-Fitting: 2 folds, 1 reps  #>  ML Method: outcome (ranger, R2 = 27.255%), treatment (ranger, R2 = 11.206%) #>  Tuning: dirty  #>  #> Average Treatment Effect:  #>  #>     Estimate Std. Error t value  P(>|t|)     #> ate   7847.4     1269.6  6.1812 6.36e-10 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Group Average Treatment Effect:  #>  #>         Estimate Std. Error t value   P(>|t|)     #> gate.q1   4451.9      803.1  5.5434 2.967e-08 *** #> gate.q2   2102.0     1379.6  1.5236 0.1275979     #> gate.q3   7040.3     1844.2  3.8176 0.0001347 *** #> gate.q4  17793.1     4447.5  4.0007 6.316e-05 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Note: DML estimates combined using the mean method. coef(dml.401k) #>       ate   gate.q1   gate.q2   gate.q3   gate.q4  #>  7847.391  4451.856  2102.042  7040.339 17793.107  coef(dml.401k, combine.method = \"mean\") #>       ate   gate.q1   gate.q2   gate.q3   gate.q4  #>  7847.391  4451.856  2102.042  7040.339 17793.107  se(dml.401k) #>       ate   gate.q1   gate.q2   gate.q3   gate.q4  #> 1269.5504  803.0969 1379.6158 1844.1637 4447.5087  confint(dml.401k, combine.method = \"mean\") #>             2.5 %    97.5 % #> ate     5359.1184 10335.665 #> gate.q1 2877.8153  6025.897 #> gate.q2 -601.9551  4806.040 #> gate.q3 3425.8451 10654.834 #> gate.q4 9076.1498 26510.064"}]
